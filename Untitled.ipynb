{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.envs.registration import registry, register, make, spec\n",
    "import Localizer_env\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal, MultivariateNormal\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import network and agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SAC_LSTM import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorboard/compat/__init__.py\u001b[0m in \u001b[0;36mtf\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnotf\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'notf' from 'tensorboard.compat' (/home/mstaps/anaconda3/lib/python3.7/site-packages/tensorboard/compat/__init__.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mstaps/anaconda3/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'obs1': array([106.        , 112.        ,  82.73451517,  -0.13295539,\n",
       "          -0.99112202]),\n",
       "  'obs2': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]])},\n",
       " -1,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_name = 'Local-v0'\n",
    "seed = 0\n",
    "n_episodes = 100\n",
    "GAMMA = 0.99\n",
    "TAU = 1e-2\n",
    "BUFFER_SIZE = int(1e6)\n",
    "BATCH_SIZE = 256\n",
    "LR_ACTOR = 5e-4\n",
    "LR_CRITIC = 5e-4\n",
    "# FIXED_ALPHA = \n",
    "# saved_model\n",
    "\n",
    "t0 = time.time()\n",
    "writer = SummaryWriter(\"runs/\")\n",
    "env = gym.make(env_name)\n",
    "\n",
    "env.reset()\n",
    "\n",
    "desired_action = np.array([0,0])\n",
    "env.step(desired_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space_2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs 2 shape: (200, 200)\n",
      "input_size_conv_out  582\n",
      "256\n",
      "128\n",
      "386\n",
      "128\n",
      "386\n",
      "128\n",
      "386\n",
      "128\n",
      "386\n",
      "128\n",
      "Episode 2 Reward: -5.00  Average100 Score: -76.50.00   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n",
      "   rewards torch.Size([256, 1]), dones torch.Size([256, 1]), Q_target_next torch.Size([256, 1])\n",
      "log_pis_next torch.Size([256, 1]) \n",
      "Q_1 shape torch.Size([256, 1]), Q_1_targets shape torch.Size([256, 1]), Q_target1_next shape torch.Size([256, 1]) \n"
     ]
    }
   ],
   "source": [
    "action_high = env.action_space.high[0]\n",
    "action_low = env.action_space.low[0]\n",
    "torch.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "np.random.seed(seed)\n",
    "state_size = dict()\n",
    "state_size['state_1'] = env.observation_space_1.shape[0]\n",
    "state_size['state_2'] = env.observation_space_2.shape[0]\n",
    "\n",
    "print(\"obs 2 shape:\",env.observation_space_2.shape)\n",
    "action_size = env.action_space.shape[0]\n",
    "\n",
    "NN_size = dict()\n",
    "NN_size = {'sequence_length': int(50),\n",
    "           'hidden_1_size': 128,\n",
    "           'hidden_1_num_layers': 4,\n",
    "           'input_channels': 1,\n",
    "           'hidden_channels': 1,\n",
    "           'output_channels': 1,\n",
    "           'kernel_size': 3,\n",
    "           'stride': 2,\n",
    "           'hidden_2_size': 256,\n",
    "           'hidden_2_out': 64,\n",
    "           'hidden_2_num_layers': 4,\n",
    "           'input_dims_last': 256,\n",
    "           'input_dims_last critic': 386,\n",
    "           'hidden_3_size': 128,\n",
    "           'hidden_3_out': 128,\n",
    "           'hidden_3_num_layers': 3,\n",
    "}\n",
    "\n",
    "\n",
    "agent = Agent(state_size=state_size, action_size=action_size, random_seed=seed, NN_size=NN_size, action_prior=\"uniform\") \n",
    "\n",
    "Batch_1_size = 1\n",
    "\n",
    "def SAC(n_episodes=200, max_t=500, print_every=10):\n",
    "    #\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    average_100_scores = []\n",
    "\n",
    "    state_1 = np.zeros((Batch_1_size, NN_size['sequence_length'], state_size['state_1']))\n",
    "    state_2 = np.zeros((Batch_1_size, NN_size['sequence_length'], state_size['state_2'], state_size['state_2'])) \n",
    "    \n",
    "    action_buffer = np.zeros((Batch_1_size, NN_size['sequence_length'], 2))\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        #\n",
    "        state = env.reset()\n",
    "        \n",
    "        state_1 = np.zeros((Batch_1_size, NN_size['sequence_length'], state_size['state_1']))\n",
    "        \n",
    "        state_2 = np.zeros((Batch_1_size, NN_size['sequence_length'], state_size['state_2'], state_size['state_2'])) \n",
    "        \n",
    "        action_buffer = np.zeros((Batch_1_size, NN_size['sequence_length'], 2))\n",
    "        \n",
    "\n",
    "        # add next state to the sequence\n",
    "        state_1[0,-1] = state['obs1']\n",
    "        state_2[0,-1] = state['obs2']\n",
    "        \n",
    "        next_state_1 = state_1\n",
    "        next_state_2 = state_2\n",
    "        \n",
    "        \n",
    "        score = 0\n",
    "        \n",
    "        for t in range(max_t):\n",
    "            \n",
    "            # roll the state\n",
    "            state_1 = np.roll(state_1, -1, axis = 1)\n",
    "            state_2 = np.roll(state_2, -1, axis = 1)\n",
    "            \n",
    "            state_1[0,-1] = state['obs1']\n",
    "            state_2[0,-1] = state['obs2']\n",
    "                       \n",
    "            #\n",
    "            action = agent.act(state_1, state_2)\n",
    "            action_v = action.numpy()\n",
    "            action_v = action_v.reshape(2)\n",
    "            action_v = np.clip(action_v*action_high, action_low, action_high)\n",
    "            \n",
    "            next_state, reward, done, info = env.step(action_v)\n",
    "            \n",
    "\n",
    "            x_obs = next_state['obs1']\n",
    "            \n",
    "            next_state_1[0,-1] = next_state['obs1']\n",
    "            next_state_2[0,-1] = next_state['obs2']\n",
    "\n",
    "            \n",
    "            agent.step(state_1, state_2, action_v, reward, next_state_1, next_state_2, done, t)\n",
    "            \n",
    "            state_1, state_2 = next_state_1, next_state_2\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            score += reward\n",
    "#             if( t%10 ==0):\n",
    "#                 print(f'episode number {i_episode} sim_time = {t}', end='\\r')\n",
    "            if done:\n",
    "                break \n",
    "        \n",
    "        scores_deque.append(score)\n",
    "        writer.add_scalar(\"Reward\", score, i_episode)\n",
    "        writer.add_scalar(\"average_X\", np.mean(scores_deque), i_episode)\n",
    "        average_100_scores.append(np.mean(scores_deque))\n",
    "        \n",
    "        print('\\rEpisode {} Reward: {:.2f}  Average100 Score: {:.2f}'.format(i_episode, score, np.mean(scores_deque)), end=\"\")\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}  Reward: {:.2f}  Average100 Score: {:.2f}'.format(i_episode, score, np.mean(scores_deque)))\n",
    "            \n",
    "    \n",
    "    torch.save(agent.actor_local.state_dict(), args.info + \".pt\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def play():\n",
    "    agent.actor_local.eval()\n",
    "    for i_episode in range(1):\n",
    "\n",
    "        state = env.reset()\n",
    "        \n",
    "        state_1 = state['obs1']\n",
    "        state_2 = state['obs2']\n",
    "        \n",
    "        state_1 = state_1.reshape((-1,state_size['state_1']))\n",
    "        state_2 = state_2.reshape((-1,state_size['state_2'], state_size['state_2']))\n",
    "\n",
    "        while True:\n",
    "            #\n",
    "            action = agent.act(state_1, state_2)\n",
    "            \n",
    "            #\n",
    "            action_v = action[0].numpy()\n",
    "            \n",
    "            # \n",
    "            action_v = np.clip(action_v*action_high, action_low, action_high)\n",
    "            \n",
    "            next_state, reward, done, info = env.step(action_v)\n",
    "            \n",
    "            next_state_1 = next_state['obs1']\n",
    "            next_state_2 = next_state['obs2']\n",
    "            \n",
    "            next_state_1 = next_state_1.reshape((-1, state_size['state_1']))\n",
    "            next_state_2 = next_state_2.reshape((-1, state_size['state_2']))\n",
    "            \n",
    "            state_1 = next_state_1\n",
    "            state_2 = next_state_2\n",
    "            if done:\n",
    "                break \n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "SAC(n_episodes = n_episodes, max_t=500, print_every=100)\n",
    "t1 = time.time()\n",
    "\n",
    "env.close()\n",
    "print(\"training took {} min!\".format((t1-t0)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs 2 shape: (200, 200)\n",
      "input_size_conv_out  582\n",
      "256\n",
      "128\n",
      "386\n",
      "128\n",
      "386\n",
      "128\n",
      "386\n",
      "128\n",
      "386\n",
      "128\n",
      "Episode 1  Reward: -148.00  Average100 Score: -148.00\n",
      "Episode 2  Reward: -5.00  Average100 Score: -76.50\n",
      "Episode 3  Reward: -164.00  Average100 Score: -105.67\n",
      "Episode 4  Reward: -49.00  Average100 Score: -91.50\n"
     ]
    }
   ],
   "source": [
    "action_high = env.action_space.high[0]\n",
    "action_low = env.action_space.low[0]\n",
    "torch.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "np.random.seed(seed)\n",
    "state_size = dict()\n",
    "state_size['state_1'] = env.observation_space_1.shape[0]\n",
    "state_size['state_2'] = env.observation_space_2.shape[0]\n",
    "\n",
    "print(\"obs 2 shape:\",env.observation_space_2.shape)\n",
    "action_size = env.action_space.shape[0]\n",
    "\n",
    "NN_size = dict()\n",
    "NN_size = {'sequence_length': int(50),\n",
    "           'hidden_1_size': 128,\n",
    "           'hidden_1_num_layers': 4,\n",
    "           'input_channels': 1,\n",
    "           'hidden_channels': 1,\n",
    "           'output_channels': 1,\n",
    "           'kernel_size': 3,\n",
    "           'stride': 2,\n",
    "           'hidden_2_size': 256,\n",
    "           'hidden_2_out': 64,\n",
    "           'hidden_2_num_layers': 4,\n",
    "           'input_dims_last': 256,\n",
    "           'input_dims_last critic': 386,\n",
    "           'hidden_3_size': 128,\n",
    "           'hidden_3_out': 128,\n",
    "           'hidden_3_num_layers': 3,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n_episodes=200\n",
    "max_t=500\n",
    "print_every=1\n",
    "\n",
    "agent = Agent(state_size=state_size, action_size=action_size, random_seed=seed, NN_size=NN_size, action_prior=\"uniform\") \n",
    "Batch_1_size = 1\n",
    "scores_deque = deque(maxlen=100)\n",
    "average_100_scores = []\n",
    "\n",
    "state_1 = np.zeros((Batch_1_size, NN_size['sequence_length'], state_size['state_1']))\n",
    "state_2 = np.zeros((Batch_1_size, NN_size['sequence_length'], state_size['state_2'], state_size['state_2'])) \n",
    "\n",
    "action_buffer = np.zeros((Batch_1_size, NN_size['sequence_length'], 2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i_episode in range(1, n_episodes+1):\n",
    "    #\n",
    "    state = env.reset()\n",
    "    \n",
    "    state_1 = np.zeros((Batch_1_size, NN_size['sequence_length'], state_size['state_1']))\n",
    "    \n",
    "    state_2 = np.zeros((Batch_1_size, NN_size['sequence_length'], state_size['state_2'], state_size['state_2'])) \n",
    "    \n",
    "    action_buffer = np.zeros((Batch_1_size, NN_size['sequence_length'], 2))\n",
    "    \n",
    "\n",
    "    # add next state to the sequence\n",
    "    state_1[0,-1] = state['obs1']\n",
    "    state_2[0,-1] = state['obs2']\n",
    "    \n",
    "    next_state_1 = state_1\n",
    "    next_state_2 = state_2\n",
    "    \n",
    "    \n",
    "    score = 0\n",
    "    \n",
    "    for t in range(max_t):\n",
    "        \n",
    "        # roll the state\n",
    "        state_1 = np.roll(state_1, -1, axis = 1)\n",
    "        state_2 = np.roll(state_2, -1, axis = 1)\n",
    "        \n",
    "        state_1[0,-1] = state['obs1']\n",
    "        state_2[0,-1] = state['obs2']\n",
    "                   \n",
    "        #\n",
    "        action = agent.act(state_1, state_2)\n",
    "        action_v = action.numpy()\n",
    "        action_v = action_v.reshape(2)\n",
    "        action_v = np.clip(action_v*action_high, action_low, action_high)\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action_v)\n",
    "        \n",
    "\n",
    "        x_obs = next_state['obs1']\n",
    "        \n",
    "        next_state_1[0,-1] = next_state['obs1']\n",
    "        next_state_2[0,-1] = next_state['obs2']\n",
    "\n",
    "        \n",
    "        agent.step(state_1, state_2, action_v, reward, next_state_1, next_state_2, done, t)\n",
    "        \n",
    "        state_1, state_2 = next_state_1, next_state_2\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        score += reward\n",
    "#             if( t%10 ==0):\n",
    "#                 print(f'episode number {i_episode} sim_time = {t}', end='\\r')\n",
    "        if done:\n",
    "            break \n",
    "    \n",
    "    scores_deque.append(score)\n",
    "    writer.add_scalar(\"Reward\", score, i_episode)\n",
    "    writer.add_scalar(\"average_X\", np.mean(scores_deque), i_episode)\n",
    "    average_100_scores.append(np.mean(scores_deque))\n",
    "    \n",
    "#     print('\\rEpisode {} Reward: {:.2f}  Average100 Score: {:.2f}'.format(i_episode, score, np.mean(scores_deque)), end=\"\")\n",
    "    if i_episode % print_every == 0:\n",
    "        print('\\rEpisode {}  Reward: {:.2f}  Average100 Score: {:.2f}'.format(i_episode, score, np.mean(scores_deque)))\n",
    "        \n",
    "\n",
    "torch.save(agent.actor_local.state_dict(), args.info + \".pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
